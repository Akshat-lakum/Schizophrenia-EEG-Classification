{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":271524,"sourceType":"datasetVersion","datasetId":4369},{"sourceId":414249,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":338139,"modelId":359093}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:34:12.956931Z","iopub.execute_input":"2025-05-13T13:34:12.957341Z","iopub.status.idle":"2025-05-13T13:34:13.031978Z","shell.execute_reply.started":"2025-05-13T13:34:12.957313Z","shell.execute_reply":"2025-05-13T13:34:13.031222Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/button-tone-sz/demographic.csv')\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:00:17.089821Z","iopub.execute_input":"2025-05-13T13:00:17.090180Z","iopub.status.idle":"2025-05-13T13:00:17.104451Z","shell.execute_reply.started":"2025-05-13T13:00:17.090148Z","shell.execute_reply":"2025-05-13T13:00:17.103569Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T09:58:45.730246Z","iopub.execute_input":"2025-05-13T09:58:45.730584Z","iopub.status.idle":"2025-05-13T09:58:45.747897Z","shell.execute_reply.started":"2025-05-13T09:58:45.730560Z","shell.execute_reply":"2025-05-13T09:58:45.746553Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T09:58:48.513283Z","iopub.execute_input":"2025-05-13T09:58:48.513569Z","iopub.status.idle":"2025-05-13T09:58:48.535667Z","shell.execute_reply.started":"2025-05-13T09:58:48.513550Z","shell.execute_reply":"2025-05-13T09:58:48.534582Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Manually Feature Extraction","metadata":{}},{"cell_type":"code","source":"df_18 = pd.read_csv('/kaggle/input/button-tone-sz/18.csv/18.csv',header =None)\ndf_time = pd.read_csv('/kaggle/input/button-tone-sz/time.csv')\ndf_column = pd.read_csv('/kaggle/input/button-tone-sz/columnLabels.csv')\ndf_demographic = pd.read_csv('/kaggle/input/button-tone-sz/demographic.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:00:22.218732Z","iopub.execute_input":"2025-05-13T13:00:22.219044Z","iopub.status.idle":"2025-05-13T13:00:31.608188Z","shell.execute_reply.started":"2025-05-13T13:00:22.219020Z","shell.execute_reply":"2025-05-13T13:00:31.607196Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df_demographic.columns.tolist())\n# Clean column names\ndf_demographic.columns = df_demographic.columns.str.strip() # group has a leading space so we have to clean this \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:00:35.117946Z","iopub.execute_input":"2025-05-13T13:00:35.118282Z","iopub.status.idle":"2025-05-13T13:00:35.123818Z","shell.execute_reply.started":"2025-05-13T13:00:35.118257Z","shell.execute_reply":"2025-05-13T13:00:35.122906Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Assign column names to df_18 using df_column\ndf_18.columns = df_column.columns\n\n# Step 2: Merge with df_time on 'sample' to add time in milliseconds\ndf_18 = df_18.merge(df_time, on='sample', how='left')\n\n# Step 3: Extract metadata for subject 18 from df_demographic\nsubject_id = 18\nsubject_meta = df_demographic[df_demographic['subject'] == subject_id].iloc[0]\n\n# Step 4: Add subject-level info (metadata) to every row in df_18\ndf_18['subject'] = subject_id\ndf_18['group'] = subject_meta['group']         # 0 = Control, 1 = Schizophrenia\ndf_18['age'] = subject_meta['age']\ndf_18['gender'] = subject_meta['gender']\ndf_18['education'] = subject_meta['education']\n\n# Show result\ndf_18.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:00:39.308053Z","iopub.execute_input":"2025-05-13T13:00:39.308383Z","iopub.status.idle":"2025-05-13T13:00:39.646544Z","shell.execute_reply.started":"2025-05-13T13:00:39.308357Z","shell.execute_reply":"2025-05-13T13:00:39.645567Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Choose EEG channels (excluding meta columns)\neeg_channels = df_column.columns[4:]  # Skip ['subject', 'trial', 'condition', 'sample']\n\n# Step 2: Create a dictionary to hold each trial's EEG matrix\ntrials = {}\n\n# Step 3: Group df_18 by trial\nfor trial_num, trial_df in df_18.groupby('trial'):\n    eeg_data = trial_df[eeg_channels].values.T  # Shape: [channels x time]\n    trials[int(trial_num)] = eeg_data\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:00:49.149486Z","iopub.execute_input":"2025-05-13T13:00:49.150080Z","iopub.status.idle":"2025-05-13T13:00:49.575492Z","shell.execute_reply.started":"2025-05-13T13:00:49.150049Z","shell.execute_reply":"2025-05-13T13:00:49.574419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print shape of one trial (e.g., trial 1)\nprint(\"Trial 1 EEG shape:\", trials[1].shape)  # Example: (64 channels, 3072 time points)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:00:58.188644Z","iopub.execute_input":"2025-05-13T13:00:58.188995Z","iopub.status.idle":"2025-05-13T13:00:58.193556Z","shell.execute_reply.started":"2025-05-13T13:00:58.188971Z","shell.execute_reply":"2025-05-13T13:00:58.192678Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pywt\nfrom scipy.stats import skew, kurtosis\nfrom scipy.signal import welch\n\n# Simulate a single EEG trial shape (70 channels x 6144 samples)\nnp.random.seed(42)\neeg_data = np.random.randn(70, 6144)\n\n# Frequency bands (Hz)\nbands = {\n    \"delta\": (0.5, 4),\n    \"theta\": (4, 8),\n    \"alpha\": (8, 12),\n    \"beta\": (12, 30),\n    \"gamma\": (30, 100),\n}\n\n# Sampling rate (from time.csv: ~0.976 ms/sample → ~1024 Hz)\nfs = 1024  \n\ndef extract_features(eeg_trial, fs):\n    features = []\n    for channel_data in eeg_trial:\n        # Statistical features\n        features += [\n            np.mean(channel_data),\n            np.std(channel_data),\n            skew(channel_data),\n            kurtosis(channel_data),\n            np.min(channel_data),\n            np.max(channel_data),\n            np.median(channel_data),\n        ]\n\n        # Spectral features using Welch PSD\n        freqs, psd = welch(channel_data, fs)\n        for band in bands.values():\n            idx = np.logical_and(freqs >= band[0], freqs <= band[1])\n            features.append(np.sum(psd[idx]))\n\n        # Wavelet features (using 'db4', 5-level decomposition)\n        coeffs = pywt.wavedec(channel_data, 'db4', level=5)\n        for c in coeffs:\n            features.append(np.mean(c))\n            features.append(np.std(c))\n    return features\n\n# Extract features from the simulated EEG trial\ntrial_features = extract_features(eeg_data, fs)\n\n# Generate feature column names\nfeature_names = []\nfor ch in range(70):\n    feature_names += [\n        f\"ch{ch}_mean\", f\"ch{ch}_std\", f\"ch{ch}_skew\", f\"ch{ch}_kurt\", \n        f\"ch{ch}_min\", f\"ch{ch}_max\", f\"ch{ch}_median\"\n    ]\n    feature_names += [f\"ch{ch}_{band}_power\" for band in bands]\n    for w in range(6):  # 6 wavelet coefficient sets from level 5 decomposition\n        feature_names.append(f\"ch{ch}_w{w}_mean\")\n        feature_names.append(f\"ch{ch}_w{w}_std\")\n\n# Convert features to DataFrame and add dummy label\ndf_features = pd.DataFrame([trial_features], columns=feature_names)\ndf_features[\"label\"] = 1  # Example label (1 = Schizophrenia)\n\n# Save to CSV (Kaggle-safe path)\ncsv_path = \"./eeg_features_example.csv\"\ndf_features.to_csv(csv_path, index=False)\n\nprint(f\"CSV file saved at: {csv_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:01:19.958527Z","iopub.execute_input":"2025-05-13T13:01:19.958829Z","iopub.status.idle":"2025-05-13T13:01:20.461425Z","shell.execute_reply.started":"2025-05-13T13:01:19.958805Z","shell.execute_reply":"2025-05-13T13:01:20.460560Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Section: Helper - Generate Column Names\ndef generate_column_names():\n    names = []\n    for ch in range(70):\n        names += [f\"ch{ch}_mean\", f\"ch{ch}_std\", f\"ch{ch}_skew\", f\"ch{ch}_kurt\", f\"ch{ch}_min\", f\"ch{ch}_max\", f\"ch{ch}_median\"]\n        names += [f\"ch{ch}_{band}_power\" for band in bands]\n        names += [f\"ch{ch}_w{w}_mean\" for w in range(6)]\n        names += [f\"ch{ch}_w{w}_std\" for w in range(6)]\n    return names\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:02:04.255195Z","iopub.execute_input":"2025-05-13T13:02:04.255775Z","iopub.status.idle":"2025-05-13T13:02:04.261729Z","shell.execute_reply.started":"2025-05-13T13:02:04.255710Z","shell.execute_reply":"2025-05-13T13:02:04.260773Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_features = []\nlabels = []\nsubject_ids = []\n\ntrial_length = 6144  # Number of rows per trial\nvalid_subjects = range(1, 82)\n\nfor subject_id in valid_subjects:\n    file_path = f\"/kaggle/input/button-tone-sz/{subject_id}.csv/{subject_id}.csv\"\n    \n    if not os.path.exists(file_path):\n        continue\n\n    try:\n        df = pd.read_csv(file_path, header=None)\n        data = df.values[:, 4:]  # Remove metadata: keep only EEG channels (cols 4 onward)\n\n        # Reshape into trials\n        total_samples = data.shape[0]\n        n_trials = total_samples // trial_length\n        data = data[:n_trials * trial_length]  # Truncate overflow\n        trials = data.reshape(n_trials, trial_length, -1)  # shape: (n_trials, 6144, 70)\n\n        for trial in trials:\n            trial = trial.T  # Shape it as (channels, time) = (70, 6144)\n            features = extract_features(trial, fs)\n            all_features.append(features)\n            labels.append(df_demographic[df_demographic[\"subject\"] == subject_id][\"group\"].values[0])\n            subject_ids.append(subject_id)\n    \n    except Exception as e:\n        print(f\"❌ Error for subject {subject_id}: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:34:22.516630Z","iopub.execute_input":"2025-05-13T13:34:22.516961Z","iopub.status.idle":"2025-05-13T14:00:55.478613Z","shell.execute_reply.started":"2025-05-13T13:34:22.516935Z","shell.execute_reply":"2025-05-13T14:00:55.477733Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Section: Save Full Feature Dataset to CSV\ndf_features = pd.DataFrame(all_features, columns=generate_column_names())\ndf_features[\"label\"] = labels\ndf_features[\"subject\"] = subject_ids\n\ndf_features.to_csv(\"eeg_all_features.csv\", index=False)\nprint(\"✔️ Saved as eeg_all_features.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T14:18:19.730808Z","iopub.execute_input":"2025-05-13T14:18:19.731167Z","iopub.status.idle":"2025-05-13T14:18:42.523725Z","shell.execute_reply.started":"2025-05-13T14:18:19.731139Z","shell.execute_reply":"2025-05-13T14:18:42.522821Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert features and labels to DataFrame\ndf_all_features = pd.DataFrame(all_features)\ndf_all_features[\"label\"] = labels  # Binary labels (0 = Control, 1 = Schizophrenia)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T10:41:29.195372Z","iopub.execute_input":"2025-05-13T10:41:29.195728Z","iopub.status.idle":"2025-05-13T10:41:35.156574Z","shell.execute_reply.started":"2025-05-13T10:41:29.195702Z","shell.execute_reply":"2025-05-13T10:41:35.155452Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check if features and labels were collected\nprint(\"✅ Total EEG feature vectors extracted:\", len(all_features))\nprint(\"✅ Total labels collected:\", len(labels))\n\n# Optional: Check a few labels\nprint(\"🔍 First 5 labels:\", labels[:5])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T10:41:35.165667Z","iopub.execute_input":"2025-05-13T10:41:35.166120Z","iopub.status.idle":"2025-05-13T10:41:35.186090Z","shell.execute_reply.started":"2025-05-13T10:41:35.166090Z","shell.execute_reply":"2025-05-13T10:41:35.184531Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\ndata_dir = \"/kaggle/input/button-tone-sz\"\navailable_dirs = [name for name in os.listdir(data_dir) if name.endswith(\".csv\")]\n\nvalid_subjects = []\n\nfor dirname in available_dirs:\n    try:\n        subject_id = int(dirname.replace(\".csv\", \"\"))\n        file_path = os.path.join(data_dir, dirname, f\"{subject_id}.csv\")\n        if os.path.exists(file_path):\n            valid_subjects.append(subject_id)\n    except:\n        continue  # Skip if folder name is not a number\n\nprint(f\"✅ Total valid EEG files found: {len(valid_subjects)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T10:41:39.616720Z","iopub.execute_input":"2025-05-13T10:41:39.617912Z","iopub.status.idle":"2025-05-13T10:41:39.732419Z","shell.execute_reply.started":"2025-05-13T10:41:39.617817Z","shell.execute_reply":"2025-05-13T10:41:39.731272Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ——— Build full paths to each subject’s CSV ———\neeg_files = [\n    os.path.join(data_dir, dirname, f\"{dirname.replace('.csv','')}.csv\")\n    for dirname in available_dirs\n    if os.path.exists(os.path.join(data_dir, dirname, f\"{dirname.replace('.csv','')}.csv\"))\n]\nprint(f\"✅ Total EEG files found: {len(eeg_files)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T10:41:43.354648Z","iopub.execute_input":"2025-05-13T10:41:43.355003Z","iopub.status.idle":"2025-05-13T10:41:43.362906Z","shell.execute_reply.started":"2025-05-13T10:41:43.354980Z","shell.execute_reply":"2025-05-13T10:41:43.361944Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Make sure these are not empty first\nprint(len(all_features), len(labels), len(subject_ids))  # should all be > 0\n\n# Save the data properly\ndf_features = pd.DataFrame(all_features, columns=generate_column_names())\ndf_features[\"label\"] = labels\ndf_features[\"subject\"] = subject_ids\n\ndf_features.to_csv(\"eeg_all_features.csv\", index=False)\nprint(\"✔️ Saved eeg_all_features.csv with shape:\", df_features.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T10:41:46.500322Z","iopub.execute_input":"2025-05-13T10:41:46.500669Z","iopub.status.idle":"2025-05-13T10:42:11.015338Z","shell.execute_reply.started":"2025-05-13T10:41:46.500644Z","shell.execute_reply":"2025-05-13T10:42:11.014285Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndf_all_features = pd.read_csv(\"eeg_all_features.csv\")\nX = df_all_features.drop(columns=[\"label\", \"subject\"])\ny = df_all_features[\"label\"]\n\n# Now this should work\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(f\"✅ Training samples: {len(X_train)}\")\nprint(f\"✅ Testing samples: {len(X_test)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T10:42:17.152915Z","iopub.execute_input":"2025-05-13T10:42:17.153270Z","iopub.status.idle":"2025-05-13T10:42:20.045021Z","shell.execute_reply.started":"2025-05-13T10:42:17.153244Z","shell.execute_reply":"2025-05-13T10:42:20.044039Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Load the full feature dataset (if not already loaded)\ndf_all_features = pd.read_csv(\"eeg_all_features.csv\")\n\n# Separate features (X) and target label (y)\nX = df_all_features.drop(columns=[\"label\", \"subject\"])  # Drop subject column too\ny = df_all_features[\"label\"]\n\n# Check if dataset is non-empty\nif len(df_all_features) == 0:\n    raise ValueError(\"❌ No data found in eeg_all_features.csv. Cannot split.\")\n\n# Train-test split (80% train, 20% test)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(f\"✅ Training samples: {len(X_train)}\")\nprint(f\"✅ Testing samples: {len(X_test)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T10:42:24.834789Z","iopub.execute_input":"2025-05-13T10:42:24.835189Z","iopub.status.idle":"2025-05-13T10:42:27.589041Z","shell.execute_reply.started":"2025-05-13T10:42:24.835165Z","shell.execute_reply":"2025-05-13T10:42:27.587508Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ML MODELS","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.metrics import roc_auc_score, roc_curve\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T10:42:34.212644Z","iopub.execute_input":"2025-05-13T10:42:34.213011Z","iopub.status.idle":"2025-05-13T10:42:34.219493Z","shell.execute_reply.started":"2025-05-13T10:42:34.212972Z","shell.execute_reply":"2025-05-13T10:42:34.218458Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Random forest","metadata":{}},{"cell_type":"code","source":"# Initialize Random Forest\nrf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# Train the model\nrf_clf.fit(X_train, y_train)\n\n# Predict on test set\ny_pred = rf_clf.predict(X_test)\n\n# Evaluate\nprint(\"🔍 Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"📊 Classification Report:\\n\", classification_report(y_test, y_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T10:42:37.846075Z","iopub.execute_input":"2025-05-13T10:42:37.846432Z","iopub.status.idle":"2025-05-13T10:42:58.963975Z","shell.execute_reply.started":"2025-05-13T10:42:37.846409Z","shell.execute_reply":"2025-05-13T10:42:58.962694Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\n\nplt.figure(figsize=(6, 4))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Control\", \"Schizophrenia\"], yticklabels=[\"Control\", \"Schizophrenia\"])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"🧠 Confusion Matrix - Random Forest\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T10:44:36.598421Z","iopub.execute_input":"2025-05-13T10:44:36.598868Z","iopub.status.idle":"2025-05-13T10:44:37.001957Z","shell.execute_reply.started":"2025-05-13T10:44:36.598840Z","shell.execute_reply":"2025-05-13T10:44:37.000519Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Probability scores for positive class (label=1)\ny_proba = rf_clf.predict_proba(X_test)[:, 1]\n\n# ROC Curve\nfpr, tpr, _ = roc_curve(y_test, y_proba)\nauc_score = roc_auc_score(y_test, y_proba)\n\nplt.figure(figsize=(6, 4))\nplt.plot(fpr, tpr, color=\"darkorange\", label=f\"AUC = {auc_score:.2f}\")\nplt.plot([0, 1], [0, 1], color=\"navy\", linestyle=\"--\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"📈 ROC Curve - Random Forest\")\nplt.legend(loc=\"lower right\")\nplt.grid()\nplt.show()\n\nprint(f\"✅ AUC Score: {auc_score:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T10:44:54.518010Z","iopub.execute_input":"2025-05-13T10:44:54.518346Z","iopub.status.idle":"2025-05-13T10:44:54.803917Z","shell.execute_reply.started":"2025-05-13T10:44:54.518325Z","shell.execute_reply":"2025-05-13T10:44:54.802810Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Feature Importance (Interpretability)","metadata":{}},{"cell_type":"code","source":"# Feature importance from Random Forest\nimportances = rf_clf.feature_importances_\nfeature_names = X.columns\nfeature_df = pd.DataFrame({\"Feature\": feature_names, \"Importance\": importances})\nfeature_df = feature_df.sort_values(by=\"Importance\", ascending=False)\n\n# Plot top 20 features\nplt.figure(figsize=(10, 6))\nsns.barplot(x=\"Importance\", y=\"Feature\", data=feature_df.head(20), palette=\"viridis\")\nplt.title(\"🔍 Top 20 Important Features - Random Forest\")\nplt.xlabel(\"Feature Importance\")\nplt.ylabel(\"Feature Name\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T10:47:07.730350Z","iopub.execute_input":"2025-05-13T10:47:07.730728Z","iopub.status.idle":"2025-05-13T10:47:08.112635Z","shell.execute_reply.started":"2025-05-13T10:47:07.730706Z","shell.execute_reply":"2025-05-13T10:47:08.111674Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Comparision of ML Models","metadata":{}},{"cell_type":"code","source":"# Define Evaluation Function\nfrom sklearn.metrics import accuracy_score, roc_auc_score, classification_report\nfrom sklearn.model_selection import train_test_split\nimport time\n\ndef evaluate_model(model, model_name):\n    start = time.time()\n    model.fit(X_train, y_train)\n    end = time.time()\n\n    y_pred = model.predict(X_test)\n    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else model.decision_function(X_test)\n\n    acc = accuracy_score(y_test, y_pred)\n    auc = roc_auc_score(y_test, y_prob)\n    report = classification_report(y_test, y_pred, output_dict=True)\n\n    return {\n        \"Model\": model_name,\n        \"Accuracy\": acc,\n        \"AUC\": auc,\n        \"F1\": report[\"weighted avg\"][\"f1-score\"],\n        \"Time (s)\": round(end - start, 2)\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T10:50:31.184365Z","iopub.execute_input":"2025-05-13T10:50:31.184732Z","iopub.status.idle":"2025-05-13T10:50:31.193169Z","shell.execute_reply.started":"2025-05-13T10:50:31.184709Z","shell.execute_reply":"2025-05-13T10:50:31.191631Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train all the models\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\n\n# Define all models\nmodels = [\n    (RandomForestClassifier(random_state=42), \"Random Forest\"),\n    (SVC(kernel=\"rbf\", probability=True, random_state=42), \"SVM\"),\n    (LogisticRegression(max_iter=1000, random_state=42), \"Logistic Regression\"),\n    (XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42), \"XGBoost\")\n]\n\n# Evaluate and store results\nresults = [evaluate_model(model, name) for model, name in models]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T10:50:52.647577Z","iopub.execute_input":"2025-05-13T10:50:52.647996Z","iopub.status.idle":"2025-05-13T10:53:42.668762Z","shell.execute_reply.started":"2025-05-13T10:50:52.647972Z","shell.execute_reply":"2025-05-13T10:53:42.668024Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compare All Models\n\n# Convert to DataFrame\nresults_df = pd.DataFrame(results)\n\n# Display sorted by AUC\nresults_df = results_df.sort_values(by=\"AUC\", ascending=False)\nprint(results_df)\n\n# Plot comparison\nplt.figure(figsize=(10, 6))\nsns.barplot(x=\"AUC\", y=\"Model\", data=results_df, palette=\"mako\")\nplt.title(\"📈 Model Comparison (AUC Score)\")\nplt.xlabel(\"AUC Score\")\nplt.ylabel(\"Model\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T10:53:47.565432Z","iopub.execute_input":"2025-05-13T10:53:47.565777Z","iopub.status.idle":"2025-05-13T10:53:47.803761Z","shell.execute_reply.started":"2025-05-13T10:53:47.565744Z","shell.execute_reply":"2025-05-13T10:53:47.802715Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Deep learning Model","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T11:04:41.385079Z","iopub.execute_input":"2025-05-13T11:04:41.385494Z","iopub.status.idle":"2025-05-13T11:05:01.438647Z","shell.execute_reply.started":"2025-05-13T11:04:41.385469Z","shell.execute_reply":"2025-05-13T11:05:01.437526Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T11:05:18.193219Z","iopub.execute_input":"2025-05-13T11:05:18.193534Z","iopub.status.idle":"2025-05-13T11:05:18.458132Z","shell.execute_reply.started":"2025-05-13T11:05:18.193512Z","shell.execute_reply":"2025-05-13T11:05:18.457303Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = Sequential([\n    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n    BatchNormalization(),\n    Dropout(0.3),\n\n    Dense(64, activation='relu'),\n    Dropout(0.3),\n\n    Dense(32, activation='relu'),\n\n    Dense(1, activation='sigmoid')  # Binary classification\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T11:05:22.583307Z","iopub.execute_input":"2025-05-13T11:05:22.583666Z","iopub.status.idle":"2025-05-13T11:05:22.716444Z","shell.execute_reply.started":"2025-05-13T11:05:22.583640Z","shell.execute_reply":"2025-05-13T11:05:22.715412Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T11:05:26.069976Z","iopub.execute_input":"2025-05-13T11:05:26.070286Z","iopub.status.idle":"2025-05-13T11:05:26.097009Z","shell.execute_reply.started":"2025-05-13T11:05:26.070267Z","shell.execute_reply":"2025-05-13T11:05:26.096035Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\nhistory = model.fit(\n    X_train_scaled, y_train,\n    validation_split=0.2,\n    epochs=100,\n    batch_size=32,\n    callbacks=[early_stop],\n    verbose=1\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T11:05:28.557376Z","iopub.execute_input":"2025-05-13T11:05:28.557756Z","iopub.status.idle":"2025-05-13T11:05:41.910973Z","shell.execute_reply.started":"2025-05-13T11:05:28.557732Z","shell.execute_reply":"2025-05-13T11:05:41.909994Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loss, accuracy, auc = model.evaluate(X_test_scaled, y_test)\nprint(f\"✅ Test Accuracy: {accuracy:.4f}\")\nprint(f\"📈 Test AUC: {auc:.4f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T11:06:07.418029Z","iopub.execute_input":"2025-05-13T11:06:07.418425Z","iopub.status.idle":"2025-05-13T11:06:07.619109Z","shell.execute_reply.started":"2025-05-13T11:06:07.418389Z","shell.execute_reply":"2025-05-13T11:06:07.617208Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot Accuracy\nplt.plot(history.history['accuracy'], label='Train Acc')\nplt.plot(history.history['val_accuracy'], label='Val Acc')\nplt.title('Model Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid()\nplt.show()\n\n# Plot AUC\nplt.plot(history.history['auc'], label='Train AUC')\nplt.plot(history.history['val_auc'], label='Val AUC')\nplt.title('Model AUC')\nplt.xlabel('Epochs')\nplt.ylabel('AUC')\nplt.legend()\nplt.grid()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T11:23:43.445970Z","iopub.execute_input":"2025-05-13T11:23:43.447112Z","iopub.status.idle":"2025-05-13T11:23:43.895777Z","shell.execute_reply.started":"2025-05-13T11:23:43.447068Z","shell.execute_reply":"2025-05-13T11:23:43.894689Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ny_pred_dl = model.predict(X_test).argmax(axis=1)  # for one-hot\n# or use threshold if sigmoid + binary: y_pred_dl = (model.predict(X_test) > 0.5).astype(\"int32\")\n\ncm = confusion_matrix(y_test, y_pred_dl)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot(cmap='Blues')\nplt.title(\"Confusion Matrix (Deep Learning)\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T11:24:32.163375Z","iopub.execute_input":"2025-05-13T11:24:32.163758Z","iopub.status.idle":"2025-05-13T11:24:32.652999Z","shell.execute_reply.started":"2025-05-13T11:24:32.163733Z","shell.execute_reply":"2025-05-13T11:24:32.651892Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature extraction through ML and training through DL","metadata":{}},{"cell_type":"code","source":"# Step-by-step: Feature Extraction via Autoencoder + DL Training\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# 1. Load your feature dataset\nfeatures_df = pd.read_csv(\"eeg_all_features.csv\")\nX = features_df.drop(columns=[\"label\", \"subject\"])\ny = features_df[\"label\"]\n\n# 2. Standardize features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 3. Train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X_scaled, y, test_size=0.2, stratify=y, random_state=42\n)\n\n# 4. Autoencoder model for feature learning\ninput_dim = X_train.shape[1]\nencoding_dim = 32  # compressed feature size\n\ninput_layer = Input(shape=(input_dim,))\nencoded = Dense(64, activation=\"relu\")(input_layer)\nencoded = Dense(encoding_dim, activation=\"relu\")(encoded)\n\ndecoded = Dense(64, activation=\"relu\")(encoded)\ndecoded = Dense(input_dim, activation=\"linear\")(decoded)\n\nautoencoder = Model(input_layer, decoded)\nautoencoder.compile(optimizer=Adam(1e-3), loss=\"mse\")\nautoencoder.fit(X_train, X_train,\n                epochs=50,\n                batch_size=64,\n                shuffle=True,\n                validation_split=0.2,\n                callbacks=[EarlyStopping(patience=5, restore_best_weights=True)],\n                verbose=1)\n\n# 5. Extract compressed features\nencoder = Model(input_layer, encoded)\nX_train_encoded = encoder.predict(X_train)\nX_test_encoded = encoder.predict(X_test)\n\n# 6. Train Deep Learning model using compressed features\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dropout\n\nmodel = Sequential([\n    Dense(64, activation='relu', input_shape=(encoding_dim,)),\n    Dropout(0.3),\n    Dense(32, activation='relu'),\n    Dropout(0.3),\n    Dense(1, activation='sigmoid')\n])\n\nmodel.compile(optimizer=Adam(1e-3), loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n\nhistory = model.fit(\n    X_train_encoded, y_train,\n    validation_data=(X_test_encoded, y_test),\n    epochs=50,\n    batch_size=64,\n    callbacks=[EarlyStopping(patience=5, restore_best_weights=True)],\n    verbose=1\n)\n\n# 7. Evaluate\nloss, accuracy, auc = model.evaluate(X_test_encoded, y_test, verbose=0)\nprint(f\"\\n✅ DL Model on Autoencoder Features\")\nprint(f\"Test Accuracy: {accuracy:.4f}\")\nprint(f\"Test AUC: {auc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T14:18:55.369349Z","iopub.execute_input":"2025-05-13T14:18:55.369683Z","iopub.status.idle":"2025-05-13T14:19:35.002550Z","shell.execute_reply.started":"2025-05-13T14:18:55.369658Z","shell.execute_reply":"2025-05-13T14:19:35.001564Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}